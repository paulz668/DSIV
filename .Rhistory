if (i > 100){
X <- c(X, 1)
}
else{
X <- c(X, 0)
}
}
e1 <- p %*% X
X2 <- c()
X2_i <- c()
for (i in S) {
if (i > 200){
X2 <- c(X2, i - 100)
X2_i <- c(X2_i, 1)
}
else{
X2 <- c(X2, 0)
X2_i <- c(X2_i, 0)
}
}
p2 <- sum(p*X2_i)
e2 <- p %*% X2
v2 <- p %*% (X2^2) - (p %*% X2)^2
#n=10000
n <- 10000
x0 <- 0
xn <- 50
x <- seq(x0,xn,length=n)
Delta <- x[2] - x[1]
E_gamma_10000 <- sum((dgamma(x,1,1)/(1+x)) * Delta)
E_gamma_10000
V_gamma_10000 <- sum((dgamma(x,1,1)^2)/(1+x)) * Delta - ((E_gamma_10000)^2)
V_gamma_10000
x <- seq(0, 1, length.out = n)
n <- 1000
x <- seq(0, 1, length.out = n)
delta <- x[2] - x[1]
vals <- 1/(1-log(-x))
log(-1)
vals <- 1/(1-log(-x+1))
vals * delta
sum(vals * delta)
e_g = sum(vals * delta)
n2 <- 10000
x2 <- seq(0, 1, length.out = n2)
delta2 <- x2[2] - x2[1]
vals2 <- 1/(1-log(-x2+1))
e_g2 = sum(vals2 * delta2)
vals_var <- 1/(1-log(-2*x)/2)^2
n <- 1000
x <- seq(0, 1, length.out = n)
delta <- x[2] - x[1]
vals <- 1/(1-log(-x+1))
vals_n <- exp(qnorm(x, mean = -0.5, sd = 1)) - 1
e_g <- sum(vals * delta)
e_n <- sum(vals_n * delta)
vals_n
qnorm(x, mean = -0.5, sd = 1)
qnorm(x, mean = -0.5, sd = 1)
X[1000]
x[1000]
x <- x[:-1]
x <- (1:n-1)/n
delta <- x[2] - x[1]
vals <- 1/(1-log(-x+1))
vals_n <- exp(qnorm(x, mean = -0.5, sd = 1)) - 1
e_g <- sum(vals * delta)
e_n <- sum(vals_n * delta)
n2 <- 10000
x2 <- (1:n-1)/n
delta2 <- x2[2] - x2[1]
vals2 <- 1/(1-log(-x2+1))
e_g2 = sum(vals2 * delta2)
vals_n2 <- exp(qnorm(x2, mean = -0.5, sd = 1)) - 1
e_g2 <- sum(vals2 * delta2)
e_n2 <- sum(vals_n * delta2)
e_n2 <- sum(vals_n2 * delta2)
x2 <- (1:n-1)/n2
x2 <- (1:n2-1)/n2
delta2 <- x2[2] - x2[1]
vals2 <- 1/(1-log(-x2+1))
vals_n2 <- exp(qnorm(x2, mean = -0.5, sd = 1)) - 1
e_g2 <- sum(vals2 * delta2)
e_n2 <- sum(vals_n2 * delta2)
n <- 1000
x <- (1:n-1)/n
delta <- x[2] - x[1]
vals <- 1/(1-log(-x+1))
vals_n <- exp(qnorm(x, mean = -0.5, sd = 1)) - 1
e_g <- sum(vals * delta)
e_n <- sum(vals_n * delta)
vals_gv <- -1/(1-log(-x+1))^2
v_g <- sum(vals_gv * delta) - e_g^2
n<-10
ii<-0:n
s0<-100
a<-1.1
b<-0.95
S<-s0*(a^ii)*(b^(n-ii))
p<-dbinom(ii,n,0.6)
X<-c()
for (i in S) {
if (i > 100){
X <- c(X, 1)
}
else{
X <- c(X, 0)
}
}
e1 <- p %*% X
X2 <- c()
X2_i <- c()
for (i in S) {
if (i > 200){
X2 <- c(X2, i - 100)
X2_i <- c(X2_i, 1)
}
else{
X2 <- c(X2, 0)
X2_i <- c(X2_i, 0)
}
}
p2 <- sum(p*X2_i)
e2 <- p %*% X2
v2 <- p %*% (X2^2) - (p %*% X2)^2
n <- 1000
x <- (1:n-1)/n
delta <- x[2] - x[1]
vals_g <- 1/(1-log(-x+1))
vals_gv <- -1/(1-log(-x+1))^2
e_g <- sum(vals_g * delta)
vals_n <- exp(qnorm(x, mean = -0.5, sd = 1)) - 1
e_n <- sum(vals_n * delta)
n2 <- 10000
x2 <- (1:n2-1)/n2
delta2 <- x2[2] - x2[1]
vals2 <- 1/(1-log(-x2+1))
vals_n2 <- exp(qnorm(x2, mean = -0.5, sd = 1)) - 1
e_g2 <- sum(vals2 * delta2)
e_n2 <- sum(vals_n2 * delta2)
library(recommenderlab)
#Data preprocessing for this unit
library(recommenderlab)
data("MovieLense")
ML_df <- as(MovieLense,'data.frame')
ML_df <- ML_df[-which(ML_df$item=='unknown'),]
ratings_per_movie <- aggregate(user~item,ML_df,length)
ML_df <- ML_df[!(ML_df[,'item']%in%ratings_per_movie[ratings_per_movie$user<10,'item']),]
ML_matrix <- xtabs(rating ~ user + item, data=ML_df)
# We need a true matrix format
class(ML_matrix)
## Our case uses the MovieLens dataset from the R package recommenderlab
library(recommenderlab)
data("MovieLense")
str(MovieLense)
str(MovieLenseMeta)
str(MovieLenseUser)
ML_df <- as(MovieLense,'data.frame')
head(ML_df)
colnames(MovieLenseUser)
colnames(MovieLenseMeta)
colnames(ML_df)
summary(MovieLenseMeta)
summary(MovieLenseUser)
summary(ML_df)
##Checking for NAs
MovieLenseUser[!complete.cases(MovieLenseUser),]
MovieLenseMeta[!complete.cases(MovieLenseMeta),]
which(ML_df$item=='unknown')
##Removing 'unknown'
ML_df <- ML_df[-which(ML_df$item=='unknown'),]
##Number of ratings per movie
ratings_per_movie <- aggregate(user~item,ML_df,length)
summary(ratings_per_movie$user)
# Least watched and most watched movies
head(ratings_per_movie[order(ratings_per_movie$user),])
head(ratings_per_movie[order(ratings_per_movie$user,
decreasing=T),])
#Distribution of ratings per movie
hist(ratings_per_movie$user,
main='Distribution of ratings per movie',
xlab = 'Ratings')
#Number of movies rated fewer than 10 times
sum(ratings_per_movie$user<10)
#Removing movies rated fewer than 10 times
ML_df <- ML_df[!(ML_df[,'item']%in%ratings_per_movie[ratings_per_movie$user<10,'item']),]
#Converting data to a true User x Item ratings matrix
ML_matrix <- xtabs(rating ~ user + item, data=ML_df)
ML_matrix[1:4,1:4]
dim(ML_matrix)
## How sparse is the matrix?
sum((ML_matrix>0))/length(ML_matrix)
##Ratings distribution
hist(ML_matrix[ML_matrix>0],
main='Distribution of all ratings',
xlab = 'Ratings')
##Predict how user 5 would rate 'Pulp Fiction (1994)'
#Users which have rated Pulp Fiction before
relevant_users <- which(ML_matrix[,'Pulp Fiction (1994)']>0)
#Similarities of user 5 and other users who have rated Pulp Fiction
#Note: the '5' selector is used to ensure that the row name 5 is referenced and not the actual 5th row
similarities <- cor(t(ML_matrix))['5',relevant_users]
#Getting the most similar users who have rated Pulp Fiction before
k <- 50
similarities <- (similarities[order(similarities,
decreasing = T)])[1:k]
relevant_users <- names(similarities)
#Ratings of Pulp Fiction by these users
relevant_ratings <- ML_matrix[relevant_users,
'Pulp Fiction (1994)']
#Getting user rating means
mu <- aggregate(rating~user,ML_df,mean)
rownames(mu) <- mu[,1]
relevant_mu <- mu[relevant_users,2]
user_mu <- mu['5',2]
#Predicted rating of user 5 for Pulp Fiction
round(user_mu+
(sum(similarities*(relevant_ratings-relevant_mu))/
sum(abs(similarities))))
# Running UBCF on the whole dataset
rec <- Recommender(as(ML_df,'realRatingMatrix'), method = "UBCF")
class(rec)
library(recommenderlab)
data("MovieLense")
ML_df <- as(MovieLense,'data.frame')
ML_df <- ML_df[-which(ML_df$item=='unknown'),]
rec_ubcf <- Recommender(as(ML_df,'realRatingMatrix'), method = "UBCF")
rec_ibcf <- Recommender(as(ML_df,'realRatingMatrix'), method = "IBCF")
rec_hyb <- HybridRecommender(
rec_ubcf,
rec_ibcf
)
rec_hyb
recommenders <- list(
RANDOM = list(name = "UBCF", param = NULL),
POPULAR = list(name = "IBCF", param = NULL),
)
weights <- c(.5, .5)
rec_hyb <- Recommender(
as(ML_df, 'realRatingMatrix'),
method = 'HYBRID',
parameter = list(recommenders = recommenders, weights = weights)
)
recommenders <- list(
RANDOM = list(name = "UBCF", param = NULL),
POPULAR = list(name = "IBCF", param = NULL)
)
weights <- c(.5, .5)
rec_hyb <- Recommender(
as(ML_df, 'realRatingMatrix'),
method = 'HYBRID',
parameter = list(recommenders = recommenders, weights = weights)
)
set.seed(111)
evaluate_hyb_scheme <- evaluationScheme(data = as(ML_df,'realRatingMatrix'),
method = "cross-validation",
k = 10,
given = 15,
goodRating = 4,
train=0.8)
evaluate_hyb <- evaluate(evaluate_hyb_scheme, "HYBRID", parameter = list(recommenders = recommenders, weights = weights), n=c(1,3,5,10,30,100))
evaluate_hyb_scheme <- evaluationScheme(data = as(ML_df,'realRatingMatrix'),
method = "cross-validation",
k = 10,
given = 15,
goodRating = 4,
train=0.8)
evaluate_hyb <- evaluate(evaluate_hyb_scheme, "HYBRID", parameter = list(recommenders = recommenders, weights = weights), n=c(1,3,5,10,30,100))
#Get the average accuracy data of the 10 runs
avg(evaluate_hyb)
plot(evaluate_hyb, annotate = TRUE)
Wn <- rnorm(N, mean = 0, sd = 1/N)
s0 <- 100
sig2 <- 0.2
r <- 0.01
N <- 240
Wn <- rnorm(N, mean = 0, sd = 1/N)
n <- 1:N
Sn <- s0*exp((r-sig2/2)*(n/N)*sqrt(sig2)*Wn)
Sn
Sn[-1]
Sn[-1]
Sn[240]
Sn[241]
payoff <- function(Sn, s0){
if(Sn[240]<2*min(Sn)){
return(Sn[240]/min(Sn))
}
else{
return(2*s0)
}
}
payoff(Sn, s0)
length(Sn)
payoff <- function(Sn, s0){
l <- length(Sn)
if(Sn[l]<2*min(Sn)){
return(Sn[l]/min(Sn))
}
else{
return(2*s0)
}
}
library(PerformanceAnalytics)
wn <- rnorm(N, mean = 0, sd = 1/N)
Wn <- Return.cumulative(wn)
wn[1:2]
wn[1:240]
wn[1:1]
cumulative_Zn(Zn){
l <- length(Zn)
Wn <- c()
for (i in 1:l) {
append(Wn, sum(Zn[1:i]))
}
return(Wn)
}
cumulative_Zn <- function(Zn){
l <- length(Zn)
Wn <- c()
for (i in 1:l) {
append(Wn, sum(Zn[1:i]))
}
return(Wn)
}
Wn <- cumulative_Zn(Zn)
Zn <- rnorm(N, mean = 0, sd = 1/N)
Wn <- cumulative_Zn(Zn)
x <- c()
append(x, 10)
x <- append(x, 10)
cumulative_Zn <- function(Zn){
Wn <- c()
l <- length(Zn)
for (i in 1:l) {
Wn <- append(Wn, sum(Zn[1:i]))
}
return(Wn)
}
Wn <- cumulative_Zn(Zn)
Sn <- s0*exp((r-sig2/2)*(n/N)*sqrt(sig2)*Wn)
Sn
payoff(Sn, s0)
library(recommenderlab)
data("MovieLense")
ML_df <- as(MovieLense,'data.frame')
ML_df <- ML_df[-which(ML_df$item=='unknown'),]
ratings_per_movie <- aggregate(user~item,ML_df,length)
ML_df <- ML_df[!(ML_df[,'item']%in%ratings_per_movie[ratings_per_movie$user<10,'item']),]
ML_matrix <- xtabs(rating ~ user + item, data=ML_df)
user_x_item <- apply(as.matrix.noquote(ML_matrix),2,as.numeric)
rownames(user_x_item) <- rownames(ML_matrix)
als_model <- Recommender(as(ML_df,'realRatingMatrix'),
method='ALS',
param=list(n_factors=85))
evaluate_hyb <- evaluate(evaluate_hyb_scheme, "ALS", parameter = list(n_factors = 85), n=10)
evaluate_als_scheme <- evaluationScheme(data = as(ML_df,'realRatingMatrix'),
method = "cross-validation",
k = 3,
given = 15,
goodRating = 4,
train=0.8)
evaluate_hyb <- evaluate(evaluate_hyb_scheme, "ALS", parameter = list(n_factors = 85), n=10)
evaluate_hyb <- evaluate(evaluate_als_scheme, "ALS", parameter = list(n_factors = 85), n=10)
#Get the average accuracy data of the 10 runs
avg(evaluate_hyb)
plot(evaluate_hyb, annotate = TRUE)
evaluate_als <- evaluate(evaluate_als_scheme, "ALS", parameter = list(n_factors = 85), n=10)
#Get the average accuracy data of the 3 runs
avg(evaluate_als)
plot(evaluate_als, annotate = TRUE)
f1score <- function(prec, rec){
as.numeric((2*prec*rec)/(prec+rec))
}
#Get the average accuracy data of the 3 runs
als results = avg(evaluate_als)
#Get the average accuracy data of the 3 runs
als results = avg(evaluate_als)
#Get the average accuracy data of the 3 runs
als_results = avg(evaluate_als)
f1score(als_results[4,'precision'], als_results[4,'recall'])
als_results
als_results[6]
f1score(als_results[6], als_results[7])
als_f1 = f1score(als_results[6], als_results[7])
is.na(user_x_item)
any(is.na(user_x_item))
colSums(user_x_item)
max(colSums(user_x_item))
user_x_item[1:1]
colnames(user_x_item)
plurality_voting <- function(user_x_item){
if (any(is.na(user_x_item))) {
user_x_item[is.na(user_x_item)] <- 0
}
votes <- colSums(user_x_item)
col_names <- colnames(user_x_item)
index_max <- which.max(votes)
return(col_names[index_max])
}
plurality_voting(user_x_item[20:])
plurality_voting(user_x_item[20:1144])
plurality_voting(user_x_item[20:1144])
plurality_voting(user_x_item)
colSums?
c
colSums(user_x_item)/colSums(user_x_item != 0)
average_voting <- function(user_x_item){
if (any(is.na(user_x_item))) {
user_x_item[is.na(user_x_item)] <- 0
}
avg_rating <- colSums(user_x_item)/colSums(user_x_item != 0)
index_max <- which.max(avg_rating)
return(col_names[index_max])
}
average_voting(user_x_item)
average_voting <- function(user_x_item){
if (any(is.na(user_x_item))) {
user_x_item[is.na(user_x_item)] <- 0
}
avg_rating <- colSums(user_x_item)/colSums(user_x_item != 0)
col_names <- colnames(user_x_item)
index_max <- which.max(avg_rating)
return(col_names[index_max])
}
average_voting(user_x_item)
qnorm(0.95, mean= 0, sd=10)
mu <- 0 - (-1.7*5) + (-1.7*2)
sd <- 0.4^2*(1-(-1.7/(10*0.4)))
qnorm(0.95, mu, sd)
setwd('C:\Users\paulz\Documents\UNI\BBE\6. Semester\DS IV\DSIV') #first we set our work directory
setwd('C:/Users/paulz/Documents/UNI/BBE/6. Semester/DS IV/DSIV') #first we set our work directory
ML_df <- read.csv('C:/Users/paulz/Documents/UNI/BBE/6. Semester/DS IV/DSIV/MovieLense/ML_df.csv') #import the dataset(s)
View(ML_df)
View(ML_df)
u <- read.delim("~/UNI/BBE/6. Semester/DS IV/DSIV/ml-100k/u.data", header=FALSE)
View(u)
#import the dataset(s)
ml_data <- u
View(ML_df)
View(ML_df)
colnames(ml_data) <- c('user id', 'item id', 'rating', 'timestamp')
View(ml_data)
View(ml_data)
library(lubridate)
ml_data[4] <- as_datetime(ml_data[4])
ml_data[4] <- as.POSIXct(ml_data[4])
qnorm(0.95, mu, sd)
typeof(ml_data[4])
as.POSIXct('01/01/1970')
as.POSIXct('01/01/1970 00:00:00', format='%d/%m/Y %H:%M:%S')
as.POSIXct("2017-05-21 22:00:00", format = "%Y-%m-%d %H:%M:%S")
as.POSIXct("1970-01-01 00:00:00", format = "%Y-%m-%d %H:%M:%S")
as.POSIXct("1970-01-01 00:00:00", format = "%Y-%m-%d %H:%M:%S") + ml_data[4]
typeof(unlist(ml_data[4]))
unlist(ml_data[4])
as.POSIXct("1970-01-01 00:00:00", format = "%Y-%m-%d %H:%M:%S") + unlist(ml_data[4])
ml_data[4] <- as.POSIXct("1970-01-01 00:00:00", format = "%Y-%m-%d %H:%M:%S") + unlist(ml_data[4])
View(ml_data)
View(ml_data)
ml_data[ml_data$timestamp > as.POSIXct("2000-12-31 00:00:00", format = "%Y-%m-%d %H:%M:%S")]
ml_data_trunc <- ml_data[ml_data$timestamp > as.POSIXct("2000-12-31 00:00:00", format = "%Y-%m-%d %H:%M:%S")]
View(ml_data_trunc)
View(ml_data_trunc)
ml_data_trunc <- ml_data[ml_data$timestamp < as.POSIXct("2000-12-31 00:00:00", format = "%Y-%m-%d %H:%M:%S")]
ml_data_trunc <- ml_data[ml_data$timestamp < as.POSIXct("2000-12-31 00:00:00", format = "%Y-%m-%d %H:%M:%S")]
cutoff <- as.POSIXct("2000-12-31 00:00:00", format = "%Y-%m-%d %H:%M:%S")
ml_data_trunc <- ml_data[ml_data$timestamp > cutoff,]
View(ML_df)
View(ML_df)
View(ml_data_trunc)
View(ml_data_trunc)
ml_data_trunc <- ml_data[ml_data$timestamp < cutoff,]
View(ml_data_trunc)
View(ml_data_trunc)
typeof(ml_data_trunc)
ml_data_trunc[,1]
ml_data_trunc[1,]
typeof(ml_data_trunc[1,])
library(tidyverse)
ml_data_trunc %>% group_by(user id)
#import the u.data dataset from MovieLens manually as it a .data file and i have no idea
#how to import that
# dont forget to cite this paper for movielens dataset:
# F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets:
# History and Context. ACM Transactions on Interactive Intelligent
# Systems (TiiS) 5, 4, Article 19 (December 2015), 19 pages.
# DOI=http://dx.doi.org/10.1145/2827872
ml_data <- u
colnames(ml_data) <- c('user_id', 'item_id', 'rating', 'timestamp') #set column names
ml_data[4] <- as.POSIXct("1970-01-01 00:00:00", format = "%Y-%m-%d %H:%M:%S") + unlist(ml_data[4]) #set timestamps to human readable form
cutoff <- as.POSIXct("2000-12-31 00:00:00", format = "%Y-%m-%d %H:%M:%S") #replicating the truncation of the dataset described in the paper
ml_data_trunc <- ml_data[ml_data$timestamp < cutoff,]
lower_bound <- 15 #initiate the lower and upper bounds of reviews per profile from the paper
upper_bound <- 650
ml_data_trunc %>% group_by(user id)
ml_data_trunc %>% group_by(user_id)
by_groups <- ml_data_trunc %>% group_by(user_id)
View(by_groups)
View(by_groups)
by_groups <- ml_data_trunc %>% group_by(user_id) %>% filter(lower_bound <= n() <= upper_bound)
by_groups <- ml_data_trunc %>% group_by(user_id) %>% filter(lower_bound < n() < upper_bound)
by_groups <- ml_data_trunc %>% group_by(user_id) %>% filter(n() > lower_bound)
View(by_groups)
View(by_groups)
by_groups <- by_groups %>% filter(n() < upper_bound)
View(by_groups)
View(by_groups)
ml_data_trunc <- ml_data_trunc %>% group_by(user_id) %>% filter(n() > lower_bound)
ml_data_trunc <- ml_data_trunc %>% filter(n() < upper_bound)
View(ml_data_trunc)
View(ml_data_trunc)
