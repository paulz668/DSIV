#Users which have rated Pulp Fiction before
relevant_users <- which(ML_matrix[,'Pulp Fiction (1994)']>0)
#Similarities of user 5 and other users who have rated Pulp Fiction
#Note: the '5' selector is used to ensure that the row name 5 is referenced and not the actual 5th row
similarities <- cor(t(ML_matrix))['5',relevant_users]
#Getting the most similar users who have rated Pulp Fiction before
k <- 50
similarities <- (similarities[order(similarities,
decreasing = T)])[1:k]
relevant_users <- names(similarities)
#Ratings of Pulp Fiction by these users
relevant_ratings <- ML_matrix[relevant_users,
'Pulp Fiction (1994)']
#Getting user rating means
mu <- aggregate(rating~user,ML_df,mean)
rownames(mu) <- mu[,1]
relevant_mu <- mu[relevant_users,2]
user_mu <- mu['5',2]
#Predicted rating of user 5 for Pulp Fiction
round(user_mu+
(sum(similarities*(relevant_ratings-relevant_mu))/
sum(abs(similarities))))
# Running UBCF on the whole dataset
rec <- Recommender(as(ML_df,'realRatingMatrix'), method = "UBCF")
class(rec)
library(recommenderlab)
data("MovieLense")
ML_df <- as(MovieLense,'data.frame')
ML_df <- ML_df[-which(ML_df$item=='unknown'),]
rec_ubcf <- Recommender(as(ML_df,'realRatingMatrix'), method = "UBCF")
rec_ibcf <- Recommender(as(ML_df,'realRatingMatrix'), method = "IBCF")
rec_hyb <- HybridRecommender(
rec_ubcf,
rec_ibcf
)
rec_hyb
recommenders <- list(
RANDOM = list(name = "UBCF", param = NULL),
POPULAR = list(name = "IBCF", param = NULL),
)
weights <- c(.5, .5)
rec_hyb <- Recommender(
as(ML_df, 'realRatingMatrix'),
method = 'HYBRID',
parameter = list(recommenders = recommenders, weights = weights)
)
recommenders <- list(
RANDOM = list(name = "UBCF", param = NULL),
POPULAR = list(name = "IBCF", param = NULL)
)
weights <- c(.5, .5)
rec_hyb <- Recommender(
as(ML_df, 'realRatingMatrix'),
method = 'HYBRID',
parameter = list(recommenders = recommenders, weights = weights)
)
set.seed(111)
evaluate_hyb_scheme <- evaluationScheme(data = as(ML_df,'realRatingMatrix'),
method = "cross-validation",
k = 10,
given = 15,
goodRating = 4,
train=0.8)
evaluate_hyb <- evaluate(evaluate_hyb_scheme, "HYBRID", parameter = list(recommenders = recommenders, weights = weights), n=c(1,3,5,10,30,100))
evaluate_hyb_scheme <- evaluationScheme(data = as(ML_df,'realRatingMatrix'),
method = "cross-validation",
k = 10,
given = 15,
goodRating = 4,
train=0.8)
evaluate_hyb <- evaluate(evaluate_hyb_scheme, "HYBRID", parameter = list(recommenders = recommenders, weights = weights), n=c(1,3,5,10,30,100))
#Get the average accuracy data of the 10 runs
avg(evaluate_hyb)
plot(evaluate_hyb, annotate = TRUE)
Wn <- rnorm(N, mean = 0, sd = 1/N)
s0 <- 100
sig2 <- 0.2
r <- 0.01
N <- 240
Wn <- rnorm(N, mean = 0, sd = 1/N)
n <- 1:N
Sn <- s0*exp((r-sig2/2)*(n/N)*sqrt(sig2)*Wn)
Sn
Sn[-1]
Sn[-1]
Sn[240]
Sn[241]
payoff <- function(Sn, s0){
if(Sn[240]<2*min(Sn)){
return(Sn[240]/min(Sn))
}
else{
return(2*s0)
}
}
payoff(Sn, s0)
length(Sn)
payoff <- function(Sn, s0){
l <- length(Sn)
if(Sn[l]<2*min(Sn)){
return(Sn[l]/min(Sn))
}
else{
return(2*s0)
}
}
library(PerformanceAnalytics)
wn <- rnorm(N, mean = 0, sd = 1/N)
Wn <- Return.cumulative(wn)
wn[1:2]
wn[1:240]
wn[1:1]
cumulative_Zn(Zn){
l <- length(Zn)
Wn <- c()
for (i in 1:l) {
append(Wn, sum(Zn[1:i]))
}
return(Wn)
}
cumulative_Zn <- function(Zn){
l <- length(Zn)
Wn <- c()
for (i in 1:l) {
append(Wn, sum(Zn[1:i]))
}
return(Wn)
}
Wn <- cumulative_Zn(Zn)
Zn <- rnorm(N, mean = 0, sd = 1/N)
Wn <- cumulative_Zn(Zn)
x <- c()
append(x, 10)
x <- append(x, 10)
cumulative_Zn <- function(Zn){
Wn <- c()
l <- length(Zn)
for (i in 1:l) {
Wn <- append(Wn, sum(Zn[1:i]))
}
return(Wn)
}
Wn <- cumulative_Zn(Zn)
Sn <- s0*exp((r-sig2/2)*(n/N)*sqrt(sig2)*Wn)
Sn
payoff(Sn, s0)
library(recommenderlab)
data("MovieLense")
ML_df <- as(MovieLense,'data.frame')
ML_df <- ML_df[-which(ML_df$item=='unknown'),]
ratings_per_movie <- aggregate(user~item,ML_df,length)
ML_df <- ML_df[!(ML_df[,'item']%in%ratings_per_movie[ratings_per_movie$user<10,'item']),]
ML_matrix <- xtabs(rating ~ user + item, data=ML_df)
user_x_item <- apply(as.matrix.noquote(ML_matrix),2,as.numeric)
rownames(user_x_item) <- rownames(ML_matrix)
als_model <- Recommender(as(ML_df,'realRatingMatrix'),
method='ALS',
param=list(n_factors=85))
evaluate_hyb <- evaluate(evaluate_hyb_scheme, "ALS", parameter = list(n_factors = 85), n=10)
evaluate_als_scheme <- evaluationScheme(data = as(ML_df,'realRatingMatrix'),
method = "cross-validation",
k = 3,
given = 15,
goodRating = 4,
train=0.8)
evaluate_hyb <- evaluate(evaluate_hyb_scheme, "ALS", parameter = list(n_factors = 85), n=10)
evaluate_hyb <- evaluate(evaluate_als_scheme, "ALS", parameter = list(n_factors = 85), n=10)
#Get the average accuracy data of the 10 runs
avg(evaluate_hyb)
plot(evaluate_hyb, annotate = TRUE)
evaluate_als <- evaluate(evaluate_als_scheme, "ALS", parameter = list(n_factors = 85), n=10)
#Get the average accuracy data of the 3 runs
avg(evaluate_als)
plot(evaluate_als, annotate = TRUE)
f1score <- function(prec, rec){
as.numeric((2*prec*rec)/(prec+rec))
}
#Get the average accuracy data of the 3 runs
als results = avg(evaluate_als)
#Get the average accuracy data of the 3 runs
als results = avg(evaluate_als)
#Get the average accuracy data of the 3 runs
als_results = avg(evaluate_als)
f1score(als_results[4,'precision'], als_results[4,'recall'])
als_results
als_results[6]
f1score(als_results[6], als_results[7])
als_f1 = f1score(als_results[6], als_results[7])
is.na(user_x_item)
any(is.na(user_x_item))
colSums(user_x_item)
max(colSums(user_x_item))
user_x_item[1:1]
colnames(user_x_item)
plurality_voting <- function(user_x_item){
if (any(is.na(user_x_item))) {
user_x_item[is.na(user_x_item)] <- 0
}
votes <- colSums(user_x_item)
col_names <- colnames(user_x_item)
index_max <- which.max(votes)
return(col_names[index_max])
}
plurality_voting(user_x_item[20:])
plurality_voting(user_x_item[20:1144])
plurality_voting(user_x_item[20:1144])
plurality_voting(user_x_item)
colSums?
c
colSums(user_x_item)/colSums(user_x_item != 0)
average_voting <- function(user_x_item){
if (any(is.na(user_x_item))) {
user_x_item[is.na(user_x_item)] <- 0
}
avg_rating <- colSums(user_x_item)/colSums(user_x_item != 0)
index_max <- which.max(avg_rating)
return(col_names[index_max])
}
average_voting(user_x_item)
average_voting <- function(user_x_item){
if (any(is.na(user_x_item))) {
user_x_item[is.na(user_x_item)] <- 0
}
avg_rating <- colSums(user_x_item)/colSums(user_x_item != 0)
col_names <- colnames(user_x_item)
index_max <- which.max(avg_rating)
return(col_names[index_max])
}
average_voting(user_x_item)
qnorm(0.95, mean= 0, sd=10)
mu <- 0 - (-1.7*5) + (-1.7*2)
sd <- 0.4^2*(1-(-1.7/(10*0.4)))
qnorm(0.95, mu, sd)
setwd('C:\Users\paulz\Documents\UNI\BBE\6. Semester\DS IV\DSIV') #first we set our work directory
setwd('C:/Users/paulz/Documents/UNI/BBE/6. Semester/DS IV/DSIV') #first we set our work directory
ML_df <- read.csv('C:/Users/paulz/Documents/UNI/BBE/6. Semester/DS IV/DSIV/MovieLense/ML_df.csv') #import the dataset(s)
View(ML_df)
View(ML_df)
u <- read.delim("~/UNI/BBE/6. Semester/DS IV/DSIV/ml-100k/u.data", header=FALSE)
View(u)
#import the dataset(s)
ml_data <- u
View(ML_df)
View(ML_df)
colnames(ml_data) <- c('user id', 'item id', 'rating', 'timestamp')
View(ml_data)
View(ml_data)
library(lubridate)
ml_data[4] <- as_datetime(ml_data[4])
ml_data[4] <- as.POSIXct(ml_data[4])
qnorm(0.95, mu, sd)
typeof(ml_data[4])
as.POSIXct('01/01/1970')
as.POSIXct('01/01/1970 00:00:00', format='%d/%m/Y %H:%M:%S')
as.POSIXct("2017-05-21 22:00:00", format = "%Y-%m-%d %H:%M:%S")
as.POSIXct("1970-01-01 00:00:00", format = "%Y-%m-%d %H:%M:%S")
as.POSIXct("1970-01-01 00:00:00", format = "%Y-%m-%d %H:%M:%S") + ml_data[4]
typeof(unlist(ml_data[4]))
unlist(ml_data[4])
as.POSIXct("1970-01-01 00:00:00", format = "%Y-%m-%d %H:%M:%S") + unlist(ml_data[4])
ml_data[4] <- as.POSIXct("1970-01-01 00:00:00", format = "%Y-%m-%d %H:%M:%S") + unlist(ml_data[4])
View(ml_data)
View(ml_data)
ml_data[ml_data$timestamp > as.POSIXct("2000-12-31 00:00:00", format = "%Y-%m-%d %H:%M:%S")]
ml_data_trunc <- ml_data[ml_data$timestamp > as.POSIXct("2000-12-31 00:00:00", format = "%Y-%m-%d %H:%M:%S")]
View(ml_data_trunc)
View(ml_data_trunc)
ml_data_trunc <- ml_data[ml_data$timestamp < as.POSIXct("2000-12-31 00:00:00", format = "%Y-%m-%d %H:%M:%S")]
ml_data_trunc <- ml_data[ml_data$timestamp < as.POSIXct("2000-12-31 00:00:00", format = "%Y-%m-%d %H:%M:%S")]
cutoff <- as.POSIXct("2000-12-31 00:00:00", format = "%Y-%m-%d %H:%M:%S")
ml_data_trunc <- ml_data[ml_data$timestamp > cutoff,]
View(ML_df)
View(ML_df)
View(ml_data_trunc)
View(ml_data_trunc)
ml_data_trunc <- ml_data[ml_data$timestamp < cutoff,]
View(ml_data_trunc)
View(ml_data_trunc)
typeof(ml_data_trunc)
ml_data_trunc[,1]
ml_data_trunc[1,]
typeof(ml_data_trunc[1,])
library(tidyverse)
ml_data_trunc %>% group_by(user id)
#import the u.data dataset from MovieLens manually as it a .data file and i have no idea
#how to import that
# dont forget to cite this paper for movielens dataset:
# F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets:
# History and Context. ACM Transactions on Interactive Intelligent
# Systems (TiiS) 5, 4, Article 19 (December 2015), 19 pages.
# DOI=http://dx.doi.org/10.1145/2827872
ml_data <- u
colnames(ml_data) <- c('user_id', 'item_id', 'rating', 'timestamp') #set column names
ml_data[4] <- as.POSIXct("1970-01-01 00:00:00", format = "%Y-%m-%d %H:%M:%S") + unlist(ml_data[4]) #set timestamps to human readable form
cutoff <- as.POSIXct("2000-12-31 00:00:00", format = "%Y-%m-%d %H:%M:%S") #replicating the truncation of the dataset described in the paper
ml_data_trunc <- ml_data[ml_data$timestamp < cutoff,]
lower_bound <- 15 #initiate the lower and upper bounds of reviews per profile from the paper
upper_bound <- 650
ml_data_trunc %>% group_by(user id)
ml_data_trunc %>% group_by(user_id)
by_groups <- ml_data_trunc %>% group_by(user_id)
View(by_groups)
View(by_groups)
by_groups <- ml_data_trunc %>% group_by(user_id) %>% filter(lower_bound <= n() <= upper_bound)
by_groups <- ml_data_trunc %>% group_by(user_id) %>% filter(lower_bound < n() < upper_bound)
by_groups <- ml_data_trunc %>% group_by(user_id) %>% filter(n() > lower_bound)
View(by_groups)
View(by_groups)
by_groups <- by_groups %>% filter(n() < upper_bound)
View(by_groups)
View(by_groups)
ml_data_trunc <- ml_data_trunc %>% group_by(user_id) %>% filter(n() > lower_bound)
ml_data_trunc <- ml_data_trunc %>% filter(n() < upper_bound)
View(ml_data_trunc)
View(ml_data_trunc)
library(tidyverse)
library(tidyverse)
library(lubridate)
setwd('C:/Users/paulz/Documents/UNI/BBE/6. Semester/DS IV/DSIV') #first we set our work directory
#import the u.data dataset from MovieLens manually as it a .data file and i have no idea
#how to import that
# dont forget to cite this paper for movielens dataset:
# F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets:
# History and Context. ACM Transactions on Interactive Intelligent
# Systems (TiiS) 5, 4, Article 19 (December 2015), 19 pages.
# DOI=http://dx.doi.org/10.1145/2827872
ml_data <- read.delim(u.data)
#import the u.data dataset from MovieLens manually as it a .data file and i have no idea
#how to import that
# dont forget to cite this paper for movielens dataset:
# F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets:
# History and Context. ACM Transactions on Interactive Intelligent
# Systems (TiiS) 5, 4, Article 19 (December 2015), 19 pages.
# DOI=http://dx.doi.org/10.1145/2827872
ml_data <- read.delim('/ml-100k/u.data')
#import the u.data dataset from MovieLens manually as it a .data file and i have no idea
#how to import that
# dont forget to cite this paper for movielens dataset:
# F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets:
# History and Context. ACM Transactions on Interactive Intelligent
# Systems (TiiS) 5, 4, Article 19 (December 2015), 19 pages.
# DOI=http://dx.doi.org/10.1145/2827872
ml_data <- read.delim('C:/Users/paulz/Documents/UNI/BBE/6. Semester/DS IV/DSIV/ml-100k/u.data')
View(ml_data)
View(ml_data)
#import the u.data dataset from MovieLens manually as it a .data file and i have no idea
#how to import that
# dont forget to cite this paper for movielens dataset:
# F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets:
# History and Context. ACM Transactions on Interactive Intelligent
# Systems (TiiS) 5, 4, Article 19 (December 2015), 19 pages.
# DOI=http://dx.doi.org/10.1145/2827872
ml_data <- read.delim('C:/Users/paulz/Documents/UNI/BBE/6. Semester/DS IV/DSIV/ml-100k/u.data', header=FALSE)
View(ml_data)
View(ml_data)
#set column names
colnames(ml_data) <- c('user_id', 'item_id', 'rating', 'timestamp')
#set timestamps to human readable format
ml_data[4] <- as.POSIXct("1970-01-01 00:00:00", format = "%Y-%m-%d %H:%M:%S") + unlist(ml_data[4])
#replicating the truncation of the dataset described in the paper
cutoff <- as.POSIXct("2000-12-31 00:00:00", format = "%Y-%m-%d %H:%M:%S")
ml_data_trunc <- ml_data[ml_data$timestamp < cutoff,]
#initiate the lower and upper bounds of reviews per profile from the paper
lower_bound <- 15
upper_bound <- 650
ml_data_trunc <- ml_data_trunc %>% group_by(user_id) %>% filter(n() > lower_bound)
ml_data_trunc <- ml_data_trunc %>% filter(n() < upper_bound)
View(ml_data_trunc)
View(ml_data_trunc)
typeof(ml_data_trunc)
typeof(ml_data[1,1])
typeof(ml_data[1,2])
typeof(ml_data[1,3])
typeof(ml_data[1,4])
View(ml_data_trunc)
View(ml_data_trunc)
typeof(now())
'MAE' == 'MAE'
ml_data_trunc$user_id
unique(ml_data_trunc$user_id)
ml_data_trunc[,1]
View(ml_data_trunc)
View(ml_data_trunc)
ml_data_trunc %>% filter(user_id == 196)
typeof(ml_data[1,4])) == "double"
typeof(ml_data[1,4]) == "double"
str(ml_data)
library(tidyverse)
setwd('C:/Users/paulz/Documents/UNI/BBE/6. Semester/DS IV/DSIV') #first we set our work directory
#import the u.data dataset from MovieLens manually as it a .data file and i have no idea
#how to import that
# dont forget to cite this paper for movielens dataset:
# F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets:
# History and Context. ACM Transactions on Interactive Intelligent
# Systems (TiiS) 5, 4, Article 19 (December 2015), 19 pages.
# DOI=http://dx.doi.org/10.1145/2827872
ml_data <- read.delim('C:/Users/paulz/Documents/UNI/BBE/6. Semester/DS IV/DSIV/ml-100k/u.data', header=FALSE)
#set column names
colnames(ml_data) <- c('user_id', 'item_id', 'rating', 'timestamp')
#set timestamps to human readable format
ml_data[4] <- as.POSIXct("1970-01-01 00:00:00", format = "%Y-%m-%d %H:%M:%S") + unlist(ml_data[4])
#replicating the truncation of the dataset described in the paper
cutoff <- as.POSIXct("2000-12-31 00:00:00", format = "%Y-%m-%d %H:%M:%S")
ml_data_trunc <- ml_data[ml_data$timestamp < cutoff,]
#initiate the lower and upper bounds of reviews per profile from the paper
lower_bound <- 15
upper_bound <- 650
ml_data_trunc <- ml_data_trunc %>% group_by(user_id) %>% filter(n() > lower_bound)
ml_data_trunc <- ml_data_trunc %>% filter(n() < upper_bound)
#implement ProfileMAE algorithm
ProfilError <- function(data, error = 'MAE'){
# data needs to be a list with 4 columns (user_id, item_id, rating and timestamp)
# these columns need to have entries of type integer, integer, integer and double respectively
# one can choose the profile error measure, which can be set to either MAE or MSE (default being MAE)
c <- c() #initialize vector to count the number of ratings per user
e <- c() #initialize vector to sum error terms for each user
for (u in unique(data[,1])) {
p <-
}
}
View(ml_data_trunc)
View(ml_data_trunc)
ml_data_trunc[ml_data_trunc$user_id == "196"]
ml_data_trunc[ml_data_trunc$user_id == 196]
ml_data_trunc[ml_data_trunc$user_id == 196,]
ml_data_trunc[ml_data_trunc[,1] == 196,]
p <- ml_data_trunc[ml_data_trunc[,1] == 196,]
View(p)
View(p)
p[order(p$timestamp)]
p[order(p$timestamp),]
length(ps)
length(p)
length(p[,1])
p[,1]
length(p[,1])
str(p[,1])
length(pull(p[,1]))
p[1,]
library(recommenderlab)
e <- p[1,]
e[,3]
e[,3] -2
eps <- c()
eps <- append(eps, eps[1] + 2)
eps
eps <- append(eps, 2)
eps[1] <- eps[1] +2
match.arg('MAE', c('MAE', 'MSE'))
match.arg('ME', c('MAE', 'MSE'))
colnames(ml_data_trunc)
typeof(colnames(ml_data_trunc))
colnames(ml_data_trunc) == c('User_id', 'item_id', 'rating', 'timestamp')
match.arg(colnames(ml_data), c('user_id', 'item_id', 'rating', 'timestamp'), several.ok = TRUE)
library(naivebayes)
library()$results[,1]
match.arg(c("recommenderlab", "naivebayes"), library()$results[,1])
match.arg(c("recommenderlab", "naivebayes"), library()$results[,1], several.ok = TRUE)
match.arg(c("recommenderlab", "naivebas"), library()$results[,1], several.ok = TRUE)
length(match.arg(c("recommenderlab", "naivebas"), library()$results[,1], several.ok = TRUE))
length(match.arg(c("recommenderlab", "naivebayes"), library()$results[,1], several.ok = TRUE))
eps != 4
ncol(ml_data_trunc)
e[1]
data("MovieLense")
str(MovieLense)
str(MovieLenseMeta)
str(MovieLenseUser)
ML_df <- as(MovieLense,'data.frame')
head(ML_df)
colnames(MovieLenseUser)
colnames(MovieLenseMeta)
colnames(ML_df)
summary(MovieLenseMeta)
summary(MovieLenseUser)
summary(ML_df)
##Checking for NAs
MovieLenseUser[!complete.cases(MovieLenseUser),]
MovieLenseMeta[!complete.cases(MovieLenseMeta),]
which(ML_df$item=='unknown')
##Removing 'unknown'
ML_df <- ML_df[-which(ML_df$item=='unknown'),]
##Number of ratings per movie
ratings_per_movie <- aggregate(user~item,ML_df,length)
summary(ratings_per_movie$user)
# Least watched and most watched movies
head(ratings_per_movie[order(ratings_per_movie$user),])
head(ratings_per_movie[order(ratings_per_movie$user,
decreasing=T),])
#Distribution of ratings per movie
hist(ratings_per_movie$user,
main='Distribution of ratings per movie',
xlab = 'Ratings')
#Number of movies rated fewer than 10 times
sum(ratings_per_movie$user<10)
#Removing movies rated fewer than 10 times
ML_df <- ML_df[!(ML_df[,'item']%in%ratings_per_movie[ratings_per_movie$user<10,'item']),]
View(ML_df)
View(ML_df)
# Running UBCF on the whole dataset
rec <- Recommender(as(ML_df,'realRatingMatrix'), method = "UBCF")
# Create top-5 recommendations for user 5
pre <- predict(rec, MovieLense[5], n = 5)
getList(pre)
# Running UBCF on the whole dataset
rec <- Recommender(as(ML_df,'realRatingMatrix'), method = "UBCF")
# Create top-5 recommendations for user 5
pre <- predict(rec, MovieLense[5], n = 5)
View(MovieLense)
View(MovieLense)
library(pillar)
detach("package:arules", unload = TRUE)
